{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "996b8013",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-02-17T21:14:52.450361Z",
     "iopub.status.busy": "2022-02-17T21:14:52.448761Z",
     "iopub.status.idle": "2022-02-17T21:14:52.473746Z",
     "shell.execute_reply": "2022-02-17T21:14:52.474618Z",
     "shell.execute_reply.started": "2022-02-17T21:09:24.582094Z"
    },
    "papermill": {
     "duration": 0.04774,
     "end_time": "2022-02-17T21:14:52.474994",
     "exception": false,
     "start_time": "2022-02-17T21:14:52.427254",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/tweet-sentiment-extraction/sample_submission.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/train.csv\n",
      "/kaggle/input/tweet-sentiment-extraction/test.csv\n",
      "/kaggle/input/tf-roberta/pretrained-roberta-base.h5\n",
      "/kaggle/input/tf-roberta/config-roberta-base.json\n",
      "/kaggle/input/tf-roberta/vocab-roberta-base.json\n",
      "/kaggle/input/tf-roberta/merges-roberta-base.txt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "571eea2c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:14:52.517627Z",
     "iopub.status.busy": "2022-02-17T21:14:52.517117Z",
     "iopub.status.idle": "2022-02-17T21:15:09.824227Z",
     "shell.execute_reply": "2022-02-17T21:15:09.824615Z",
     "shell.execute_reply.started": "2022-02-17T21:09:24.971842Z"
    },
    "papermill": {
     "duration": 17.329793,
     "end_time": "2022-02-17T21:15:09.824803",
     "exception": false,
     "start_time": "2022-02-17T21:14:52.495010",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF version 2.6.2\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd, numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from transformers import *\n",
    "import tokenizers\n",
    "print('TF version',tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a21ae692",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:10.003459Z",
     "iopub.status.busy": "2022-02-17T21:15:10.002356Z",
     "iopub.status.idle": "2022-02-17T21:15:10.192293Z",
     "shell.execute_reply": "2022-02-17T21:15:10.192797Z",
     "shell.execute_reply.started": "2022-02-17T21:09:52.592118Z"
    },
    "papermill": {
     "duration": 0.356545,
     "end_time": "2022-02-17T21:15:10.192951",
     "exception": false,
     "start_time": "2022-02-17T21:15:09.836406",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>selected_text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>cb774db0d1</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>I`d have responded, if I were going</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>549e992a42</td>\n",
       "      <td>Sooo SAD I will miss you here in San Diego!!!</td>\n",
       "      <td>Sooo SAD</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>088c60f138</td>\n",
       "      <td>my boss is bullying me...</td>\n",
       "      <td>bullying me</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9642c003ef</td>\n",
       "      <td>what interview! leave me alone</td>\n",
       "      <td>leave me alone</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>358bd9e861</td>\n",
       "      <td>Sons of ****, why couldn`t they put them on t...</td>\n",
       "      <td>Sons of ****,</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       textID                                               text  \\\n",
       "0  cb774db0d1                I`d have responded, if I were going   \n",
       "1  549e992a42      Sooo SAD I will miss you here in San Diego!!!   \n",
       "2  088c60f138                          my boss is bullying me...   \n",
       "3  9642c003ef                     what interview! leave me alone   \n",
       "4  358bd9e861   Sons of ****, why couldn`t they put them on t...   \n",
       "\n",
       "                         selected_text sentiment  \n",
       "0  I`d have responded, if I were going   neutral  \n",
       "1                             Sooo SAD  negative  \n",
       "2                          bullying me  negative  \n",
       "3                       leave me alone  negative  \n",
       "4                        Sons of ****,  negative  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_LEN = 96\n",
    "PATH = '../input/tf-roberta/'\n",
    "tokenizer = tokenizers.ByteLevelBPETokenizer(\n",
    "    vocab=PATH+'vocab-roberta-base.json', \n",
    "    merges=PATH+'merges-roberta-base.txt', \n",
    "    lowercase=True,\n",
    "    add_prefix_space=True\n",
    ")\n",
    "sentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}\n",
    "train = pd.read_csv('../input/tweet-sentiment-extraction/train.csv').fillna('')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b57d993",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:10.223586Z",
     "iopub.status.busy": "2022-02-17T21:15:10.222586Z",
     "iopub.status.idle": "2022-02-17T21:15:10.229704Z",
     "shell.execute_reply": "2022-02-17T21:15:10.229243Z",
     "shell.execute_reply.started": "2022-02-17T21:10:06.791926Z"
    },
    "papermill": {
     "duration": 0.025362,
     "end_time": "2022-02-17T21:15:10.229877",
     "exception": false,
     "start_time": "2022-02-17T21:15:10.204515",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ct = train.shape[0]\n",
    "input_ids = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "start_tokens = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "end_tokens = np.zeros((ct,MAX_LEN),dtype='int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b81d51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:10.264961Z",
     "iopub.status.busy": "2022-02-17T21:15:10.254785Z",
     "iopub.status.idle": "2022-02-17T21:15:19.123471Z",
     "shell.execute_reply": "2022-02-17T21:15:19.123023Z",
     "shell.execute_reply.started": "2022-02-17T21:10:29.307422Z"
    },
    "papermill": {
     "duration": 8.882555,
     "end_time": "2022-02-17T21:15:19.123596",
     "exception": false,
     "start_time": "2022-02-17T21:15:10.241041",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(train.shape[0]):\n",
    "    \n",
    "    # FIND OVERLAP\n",
    "    text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "    text2 = \" \".join(train.loc[k,'selected_text'].split())\n",
    "    idx = text1.find(text2)\n",
    "    chars = np.zeros((len(text1)))\n",
    "    chars[idx:idx+len(text2)]=1\n",
    "    if text1[idx-1]==' ': chars[idx-1] = 1 \n",
    "    enc = tokenizer.encode(text1) \n",
    "        \n",
    "    # ID_OFFSETS\n",
    "    offsets = []; idx=0\n",
    "    for t in enc.ids:\n",
    "        w = tokenizer.decode([t])\n",
    "        offsets.append((idx,idx+len(w)))\n",
    "        idx += len(w)\n",
    "    \n",
    "    # START END TOKENS\n",
    "    toks = []\n",
    "    for i,(a,b) in enumerate(offsets):\n",
    "        sm = np.sum(chars[a:b])\n",
    "        if sm>0: toks.append(i) \n",
    "        \n",
    "    s_tok = sentiment_id[train.loc[k,'sentiment']]\n",
    "    input_ids[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask[k,:len(enc.ids)+5] = 1\n",
    "    if len(toks)>0:\n",
    "        start_tokens[k,toks[0]+1] = 1\n",
    "        end_tokens[k,toks[-1]+1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "819a864d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:19.153610Z",
     "iopub.status.busy": "2022-02-17T21:15:19.153106Z",
     "iopub.status.idle": "2022-02-17T21:15:19.523988Z",
     "shell.execute_reply": "2022-02-17T21:15:19.523469Z",
     "shell.execute_reply.started": "2022-02-17T21:10:53.350570Z"
    },
    "papermill": {
     "duration": 0.388721,
     "end_time": "2022-02-17T21:15:19.524113",
     "exception": false,
     "start_time": "2022-02-17T21:15:19.135392",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('../input/tweet-sentiment-extraction/test.csv').fillna('')\n",
    "\n",
    "ct = test.shape[0]\n",
    "input_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\n",
    "attention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "token_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n",
    "\n",
    "for k in range(test.shape[0]):\n",
    "        \n",
    "    # INPUT_IDS\n",
    "    text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "    enc = tokenizer.encode(text1)                \n",
    "    s_tok = sentiment_id[test.loc[k,'sentiment']]\n",
    "    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n",
    "    attention_mask_t[k,:len(enc.ids)+5] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a61ef4f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:19.557232Z",
     "iopub.status.busy": "2022-02-17T21:15:19.556295Z",
     "iopub.status.idle": "2022-02-17T21:15:19.558204Z",
     "shell.execute_reply": "2022-02-17T21:15:19.558588Z",
     "shell.execute_reply.started": "2022-02-17T21:11:38.843333Z"
    },
    "papermill": {
     "duration": 0.023106,
     "end_time": "2022-02-17T21:15:19.558723",
     "exception": false,
     "start_time": "2022-02-17T21:15:19.535617",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "    config = RobertaConfig.from_pretrained(PATH+'config-roberta-base.json')\n",
    "    bert_model = TFRobertaModel.from_pretrained(PATH+'pretrained-roberta-base.h5',config=config)\n",
    "    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n",
    "    \n",
    "    x1 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x1 = tf.keras.layers.Conv1D(1,1)(x1)\n",
    "    x1 = tf.keras.layers.Flatten()(x1)\n",
    "    x1 = tf.keras.layers.Activation('softmax')(x1)\n",
    "    \n",
    "    x2 = tf.keras.layers.Dropout(0.1)(x[0]) \n",
    "    x2 = tf.keras.layers.Conv1D(1,1)(x2)\n",
    "    x2 = tf.keras.layers.Flatten()(x2)\n",
    "    x2 = tf.keras.layers.Activation('softmax')(x2)\n",
    "\n",
    "    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d356ad79",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:19.583949Z",
     "iopub.status.busy": "2022-02-17T21:15:19.583158Z",
     "iopub.status.idle": "2022-02-17T21:15:19.588931Z",
     "shell.execute_reply": "2022-02-17T21:15:19.588511Z",
     "shell.execute_reply.started": "2022-02-17T21:11:50.426554Z"
    },
    "papermill": {
     "duration": 0.019135,
     "end_time": "2022-02-17T21:15:19.589042",
     "exception": false,
     "start_time": "2022-02-17T21:15:19.569907",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def jaccard(str1, str2): \n",
    "    a = set(str1.lower().split()) \n",
    "    b = set(str2.lower().split())\n",
    "    if (len(a)==0) & (len(b)==0): return 0.5\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bcf5fdf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T21:15:19.628443Z",
     "iopub.status.busy": "2022-02-17T21:15:19.627530Z",
     "iopub.status.idle": "2022-02-17T22:35:29.418547Z",
     "shell.execute_reply": "2022-02-17T22:35:29.419231Z",
     "shell.execute_reply.started": "2022-02-17T21:12:10.300316Z"
    },
    "papermill": {
     "duration": 4809.81875,
     "end_time": "2022-02-17T22:35:29.419505",
     "exception": false,
     "start_time": "2022-02-17T21:15:19.600755",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### FOLD 1\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "687/687 [==============================] - 288s 399ms/step - loss: 2.1879 - activation_loss: 1.0746 - activation_1_loss: 1.1133 - val_loss: 1.7671 - val_activation_loss: 0.8915 - val_activation_1_loss: 0.8756\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.76708, saving model to v0-roberta-0.h5\n",
      "Epoch 2/3\n",
      "687/687 [==============================] - 272s 395ms/step - loss: 1.6850 - activation_loss: 0.8469 - activation_1_loss: 0.8381 - val_loss: 1.7049 - val_activation_loss: 0.8677 - val_activation_1_loss: 0.8372\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.76708 to 1.70489, saving model to v0-roberta-0.h5\n",
      "Epoch 3/3\n",
      "687/687 [==============================] - 271s 395ms/step - loss: 1.5325 - activation_loss: 0.7832 - activation_1_loss: 0.7493 - val_loss: 1.6855 - val_activation_loss: 0.8669 - val_activation_1_loss: 0.8186\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.70489 to 1.68548, saving model to v0-roberta-0.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 26s 136ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 15s 136ms/step\n",
      ">>>> FOLD 1 Jaccard = 0.7034495691086627\n",
      "\n",
      "#########################\n",
      "### FOLD 2\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "688/688 [==============================] - 301s 417ms/step - loss: 2.2104 - activation_loss: 1.0945 - activation_1_loss: 1.1160 - val_loss: 1.7346 - val_activation_loss: 0.8763 - val_activation_1_loss: 0.8582\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.73456, saving model to v0-roberta-1.h5\n",
      "Epoch 2/3\n",
      "688/688 [==============================] - 284s 413ms/step - loss: 1.6596 - activation_loss: 0.8469 - activation_1_loss: 0.8127 - val_loss: 1.6314 - val_activation_loss: 0.8372 - val_activation_1_loss: 0.7942\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.73456 to 1.63138, saving model to v0-roberta-1.h5\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 284s 413ms/step - loss: 1.5171 - activation_loss: 0.7839 - activation_1_loss: 0.7332 - val_loss: 1.6570 - val_activation_loss: 0.8470 - val_activation_1_loss: 0.8100\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.63138\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 26s 136ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 15s 137ms/step\n",
      ">>>> FOLD 2 Jaccard = 0.7043288697894092\n",
      "\n",
      "#########################\n",
      "### FOLD 3\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "688/688 [==============================] - 301s 418ms/step - loss: 2.1510 - activation_loss: 1.0656 - activation_1_loss: 1.0855 - val_loss: 1.6867 - val_activation_loss: 0.8623 - val_activation_1_loss: 0.8244\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.68669, saving model to v0-roberta-2.h5\n",
      "Epoch 2/3\n",
      "688/688 [==============================] - 285s 414ms/step - loss: 1.6747 - activation_loss: 0.8507 - activation_1_loss: 0.8240 - val_loss: 1.6449 - val_activation_loss: 0.8301 - val_activation_1_loss: 0.8148\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.68669 to 1.64487, saving model to v0-roberta-2.h5\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 284s 413ms/step - loss: 1.5713 - activation_loss: 0.7907 - activation_1_loss: 0.7806 - val_loss: 1.6777 - val_activation_loss: 0.8368 - val_activation_1_loss: 0.8409\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.64487\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 26s 136ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 15s 138ms/step\n",
      ">>>> FOLD 3 Jaccard = 0.7036624177811396\n",
      "\n",
      "#########################\n",
      "### FOLD 4\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "688/688 [==============================] - 302s 418ms/step - loss: 2.0907 - activation_loss: 1.0508 - activation_1_loss: 1.0399 - val_loss: 1.6661 - val_activation_loss: 0.8509 - val_activation_1_loss: 0.8152\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66611, saving model to v0-roberta-3.h5\n",
      "Epoch 2/3\n",
      "688/688 [==============================] - 286s 415ms/step - loss: 1.6158 - activation_loss: 0.8315 - activation_1_loss: 0.7843 - val_loss: 1.5930 - val_activation_loss: 0.8112 - val_activation_1_loss: 0.7817\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66611 to 1.59298, saving model to v0-roberta-3.h5\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 287s 417ms/step - loss: 1.4619 - activation_loss: 0.7567 - activation_1_loss: 0.7052 - val_loss: 1.6332 - val_activation_loss: 0.8178 - val_activation_1_loss: 0.8154\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 1.59298\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 28s 139ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 16s 143ms/step\n",
      ">>>> FOLD 4 Jaccard = 0.7105793013517165\n",
      "\n",
      "#########################\n",
      "### FOLD 5\n",
      "#########################\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All model checkpoint layers were used when initializing TFRobertaModel.\n",
      "\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at ../input/tf-roberta/pretrained-roberta-base.h5.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "688/688 [==============================] - 308s 424ms/step - loss: 2.1919 - activation_loss: 1.0913 - activation_1_loss: 1.1005 - val_loss: 1.6687 - val_activation_loss: 0.8523 - val_activation_1_loss: 0.8165\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.66871, saving model to v0-roberta-4.h5\n",
      "Epoch 2/3\n",
      "688/688 [==============================] - 289s 420ms/step - loss: 1.6575 - activation_loss: 0.8537 - activation_1_loss: 0.8038 - val_loss: 1.6335 - val_activation_loss: 0.8396 - val_activation_1_loss: 0.7939\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.66871 to 1.63346, saving model to v0-roberta-4.h5\n",
      "Epoch 3/3\n",
      "688/688 [==============================] - 288s 419ms/step - loss: 1.5062 - activation_loss: 0.7751 - activation_1_loss: 0.7312 - val_loss: 1.6175 - val_activation_loss: 0.8380 - val_activation_1_loss: 0.7794\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.63346 to 1.61747, saving model to v0-roberta-4.h5\n",
      "Loading model...\n",
      "Predicting OOF...\n",
      "172/172 [==============================] - 29s 141ms/step\n",
      "Predicting Test...\n",
      "111/111 [==============================] - 15s 138ms/step\n",
      ">>>> FOLD 5 Jaccard = 0.7077999258724941\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jac = []; VER='v0'; DISPLAY=1 # USE display=1 FOR INTERACTIVE\n",
    "oof_start = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "oof_end = np.zeros((input_ids.shape[0],MAX_LEN))\n",
    "preds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "preds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5,shuffle=True,random_state=777)\n",
    "for fold,(idxT,idxV) in enumerate(skf.split(input_ids,train.sentiment.values)):\n",
    "\n",
    "    print('#'*25)\n",
    "    print('### FOLD %i'%(fold+1))\n",
    "    print('#'*25)\n",
    "    \n",
    "    K.clear_session()\n",
    "    model = build_model()\n",
    "#     #print(\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "#     print(fold)\n",
    "#     print(idxT)\n",
    "#     print(idxV)\n",
    "#     continue\n",
    "    sv = tf.keras.callbacks.ModelCheckpoint(\n",
    "        '%s-roberta-%i.h5'%(VER,fold), monitor='val_loss', verbose=1, save_best_only=True,\n",
    "        save_weights_only=True, mode='auto', save_freq='epoch')\n",
    "        \n",
    "    model.fit([input_ids[idxT,], attention_mask[idxT,], token_type_ids[idxT,]], [start_tokens[idxT,], end_tokens[idxT,]], \n",
    "        epochs=3, batch_size=32, verbose=DISPLAY, callbacks=[sv],\n",
    "        validation_data=([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]], \n",
    "        [start_tokens[idxV,], end_tokens[idxV,]]))\n",
    "    \n",
    "    print('Loading model...')\n",
    "    model.load_weights('%s-roberta-%i.h5'%(VER,fold))\n",
    "    \n",
    "    print('Predicting OOF...')\n",
    "    oof_start[idxV,],oof_end[idxV,] = model.predict([input_ids[idxV,],attention_mask[idxV,],token_type_ids[idxV,]],verbose=DISPLAY)\n",
    "    \n",
    "    print('Predicting Test...')\n",
    "    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=DISPLAY)\n",
    "    preds_start += preds[0]/skf.n_splits\n",
    "    preds_end += preds[1]/skf.n_splits\n",
    "    \n",
    "    # DISPLAY FOLD JACCARD\n",
    "    all = []\n",
    "    for k in idxV:\n",
    "        a = np.argmax(oof_start[k,])\n",
    "        b = np.argmax(oof_end[k,])\n",
    "        if a>b: \n",
    "            st = train.loc[k,'text'] # IMPROVE CV/LB with better choice here\n",
    "        else:\n",
    "            text1 = \" \"+\" \".join(train.loc[k,'text'].split())\n",
    "            enc = tokenizer.encode(text1)\n",
    "            st = tokenizer.decode(enc.ids[a-1:b])\n",
    "        all.append(jaccard(st,train.loc[k,'selected_text']))\n",
    "    jac.append(np.mean(all))\n",
    "    print('>>>> FOLD %i Jaccard ='%(fold+1),np.mean(all))\n",
    "    print()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2326dd29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T22:35:37.644341Z",
     "iopub.status.busy": "2022-02-17T22:35:37.643310Z",
     "iopub.status.idle": "2022-02-17T22:35:38.047923Z",
     "shell.execute_reply": "2022-02-17T22:35:38.047323Z"
    },
    "papermill": {
     "duration": 4.605441,
     "end_time": "2022-02-17T22:35:38.048094",
     "exception": false,
     "start_time": "2022-02-17T22:35:33.442653",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all = []\n",
    "for k in range(input_ids_t.shape[0]):\n",
    "    a = np.argmax(preds_start[k,])\n",
    "    b = np.argmax(preds_end[k,])\n",
    "    if a>b: \n",
    "        st = test.loc[k,'text']\n",
    "    else:\n",
    "        text1 = \" \"+\" \".join(test.loc[k,'text'].split())\n",
    "        enc = tokenizer.encode(text1)\n",
    "        st = tokenizer.decode(enc.ids[a-1:b])\n",
    "    all.append(st)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94fb008f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-02-17T22:35:46.334049Z",
     "iopub.status.busy": "2022-02-17T22:35:46.333202Z",
     "iopub.status.idle": "2022-02-17T22:35:46.369957Z",
     "shell.execute_reply": "2022-02-17T22:35:46.370486Z"
    },
    "papermill": {
     "duration": 4.332801,
     "end_time": "2022-02-17T22:35:46.370681",
     "exception": false,
     "start_time": "2022-02-17T22:35:42.037880",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>textID</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>selected_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>769</th>\n",
       "      <td>0117a03b6d</td>\n",
       "      <td>_43 HAHHAHAH that really made me laugh out loudd! ahahah...</td>\n",
       "      <td>positive</td>\n",
       "      <td>laugh out loudd! ahahahahahahahahah your fuunny!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3326</th>\n",
       "      <td>6face8f81f</td>\n",
       "      <td>I can`t, I have an exam on wednesday</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i can`t, i have an exam on wednesday</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>480</th>\n",
       "      <td>819626535b</td>\n",
       "      <td>Glad to hear you made it out, I hear that place used to...</td>\n",
       "      <td>positive</td>\n",
       "      <td>glad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2113</th>\n",
       "      <td>ebf2a10961</td>\n",
       "      <td>I KNO!!! im so sad! evry1 is leavin its horrible! im su...</td>\n",
       "      <td>negative</td>\n",
       "      <td>sad!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2036</th>\n",
       "      <td>1bed6bec2a</td>\n",
       "      <td>Finally finished exams &amp; home not watch Otalia &amp; the PBP...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>finally finished exams &amp; home not watch otalia &amp; the pb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3092</th>\n",
       "      <td>3ed1480471</td>\n",
       "      <td>wonders if anyone would care if she died tomorrow</td>\n",
       "      <td>negative</td>\n",
       "      <td>wonders if anyone would care if she died tomorrow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3423</th>\n",
       "      <td>1d865c0717</td>\n",
       "      <td>I have the biggest headache ever. My photosensitivity is...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i have the biggest headache ever.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1973</th>\n",
       "      <td>32b129dc3f</td>\n",
       "      <td>says Happy Mother`s Day to all the Moms out there.</td>\n",
       "      <td>positive</td>\n",
       "      <td>happy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2290</th>\n",
       "      <td>c9a52dee1f</td>\n",
       "      <td>Guess I`m gonna try the nap thing again 2day, but since ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>since my kids haven`t cooperated with it yet this week ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2757</th>\n",
       "      <td>a4468f0e74</td>\n",
       "      <td>Gotcha ! Let`s meet up @ The Twitt Cafe</td>\n",
       "      <td>neutral</td>\n",
       "      <td>gotcha ! let`s meet up @ the twitt cafe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2039</th>\n",
       "      <td>9dfd003f1e</td>\n",
       "      <td>i want you to text me first everyday, not me</td>\n",
       "      <td>neutral</td>\n",
       "      <td>i want you to text me first everyday, not me</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>424</th>\n",
       "      <td>0693a85163</td>\n",
       "      <td>Allergies  sun is out, wearing shorts.  Stuck with my ad...</td>\n",
       "      <td>negative</td>\n",
       "      <td>boring.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2992</th>\n",
       "      <td>397ef3478d</td>\n",
       "      <td>Oh it is so sunny.</td>\n",
       "      <td>negative</td>\n",
       "      <td>oh it is so sunny.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>2fdd2501d1</td>\n",
       "      <td>Anyone have some advice??? I need it!!</td>\n",
       "      <td>neutral</td>\n",
       "      <td>anyone have some advice??? i need it!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>6816923abd</td>\n",
       "      <td>This month was a bad month to try and get an advert toge...</td>\n",
       "      <td>negative</td>\n",
       "      <td>bad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2191</th>\n",
       "      <td>a8e51d1a43</td>\n",
       "      <td>I miss having u as a roommate</td>\n",
       "      <td>negative</td>\n",
       "      <td>miss</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>460</th>\n",
       "      <td>3690fcb40e</td>\n",
       "      <td>http://twitpic.com/4ja8r - Tell me, how can you not love...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>http://twitpic.com/4ja8r - tell me, how can you not lov...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1502</th>\n",
       "      <td>62e1413e85</td>\n",
       "      <td>Hearing that my songs brighten someone`s day always mak...</td>\n",
       "      <td>positive</td>\n",
       "      <td>grin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2127</th>\n",
       "      <td>95189cd3a8</td>\n",
       "      <td>warmed up nicely outside.  sucks being stuck inside coding</td>\n",
       "      <td>neutral</td>\n",
       "      <td>warmed up nicely outside. sucks being stuck inside coding</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3303</th>\n",
       "      <td>93c709ddbb</td>\n",
       "      <td>a pleasure Bojan</td>\n",
       "      <td>positive</td>\n",
       "      <td>pleasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1386</th>\n",
       "      <td>51f56d7464</td>\n",
       "      <td>Still sick  bout to play some ps3 till the laker game st...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>still sick bout to play some ps3 till the laker game st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2508</th>\n",
       "      <td>8a768f4305</td>\n",
       "      <td>un cross them please..I was planning on buying a lambo ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>un cross them please..i was planning on buying a lambo ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2380</th>\n",
       "      <td>968910f054</td>\n",
       "      <td>@ MGM grand, mraz on stage! No doubling back opener.  lo...</td>\n",
       "      <td>positive</td>\n",
       "      <td>lots of old memories flooding back</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>2c150a0703</td>\n",
       "      <td>what a beautiful day not to got to my first class</td>\n",
       "      <td>positive</td>\n",
       "      <td>beautiful day</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1549</th>\n",
       "      <td>bc3032341b</td>\n",
       "      <td>i`m sad...i`ll miss you grandma angie.. you were always ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>i`m sad...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          textID                                                         text  \\\n",
       "769   0117a03b6d  _43 HAHHAHAH that really made me laugh out loudd! ahahah...   \n",
       "3326  6face8f81f                         I can`t, I have an exam on wednesday   \n",
       "480   819626535b   Glad to hear you made it out, I hear that place used to...   \n",
       "2113  ebf2a10961   I KNO!!! im so sad! evry1 is leavin its horrible! im su...   \n",
       "2036  1bed6bec2a  Finally finished exams & home not watch Otalia & the PBP...   \n",
       "3092  3ed1480471            wonders if anyone would care if she died tomorrow   \n",
       "3423  1d865c0717  I have the biggest headache ever. My photosensitivity is...   \n",
       "1973  32b129dc3f           says Happy Mother`s Day to all the Moms out there.   \n",
       "2290  c9a52dee1f  Guess I`m gonna try the nap thing again 2day, but since ...   \n",
       "2757  a4468f0e74                      Gotcha ! Let`s meet up @ The Twitt Cafe   \n",
       "2039  9dfd003f1e                 i want you to text me first everyday, not me   \n",
       "424   0693a85163  Allergies  sun is out, wearing shorts.  Stuck with my ad...   \n",
       "2992  397ef3478d                                           Oh it is so sunny.   \n",
       "388   2fdd2501d1                       Anyone have some advice??? I need it!!   \n",
       "1257  6816923abd  This month was a bad month to try and get an advert toge...   \n",
       "2191  a8e51d1a43                                I miss having u as a roommate   \n",
       "460   3690fcb40e  http://twitpic.com/4ja8r - Tell me, how can you not love...   \n",
       "1502  62e1413e85   Hearing that my songs brighten someone`s day always mak...   \n",
       "2127  95189cd3a8   warmed up nicely outside.  sucks being stuck inside coding   \n",
       "3303  93c709ddbb                                             a pleasure Bojan   \n",
       "1386  51f56d7464  Still sick  bout to play some ps3 till the laker game st...   \n",
       "2508  8a768f4305   un cross them please..I was planning on buying a lambo ...   \n",
       "2380  968910f054  @ MGM grand, mraz on stage! No doubling back opener.  lo...   \n",
       "886   2c150a0703            what a beautiful day not to got to my first class   \n",
       "1549  bc3032341b  i`m sad...i`ll miss you grandma angie.. you were always ...   \n",
       "\n",
       "     sentiment                                                selected_text  \n",
       "769   positive            laugh out loudd! ahahahahahahahahah your fuunny!!  \n",
       "3326   neutral                         i can`t, i have an exam on wednesday  \n",
       "480   positive                                                         glad  \n",
       "2113  negative                                                         sad!  \n",
       "2036   neutral   finally finished exams & home not watch otalia & the pb...  \n",
       "3092  negative            wonders if anyone would care if she died tomorrow  \n",
       "3423  negative                            i have the biggest headache ever.  \n",
       "1973  positive                                                        happy  \n",
       "2290  negative   since my kids haven`t cooperated with it yet this week ...  \n",
       "2757   neutral                      gotcha ! let`s meet up @ the twitt cafe  \n",
       "2039   neutral                 i want you to text me first everyday, not me  \n",
       "424   negative                                                      boring.  \n",
       "2992  negative                                           oh it is so sunny.  \n",
       "388    neutral                       anyone have some advice??? i need it!!  \n",
       "1257  negative                                                          bad  \n",
       "2191  negative                                                         miss  \n",
       "460    neutral   http://twitpic.com/4ja8r - tell me, how can you not lov...  \n",
       "1502  positive                                                        grin.  \n",
       "2127   neutral    warmed up nicely outside. sucks being stuck inside coding  \n",
       "3303  positive                                                     pleasure  \n",
       "1386   neutral   still sick bout to play some ps3 till the laker game st...  \n",
       "2508   neutral   un cross them please..i was planning on buying a lambo ...  \n",
       "2380  positive                           lots of old memories flooding back  \n",
       "886   positive                                                beautiful day  \n",
       "1549  negative                                                   i`m sad...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['selected_text'] = all\n",
    "test[['textID','selected_text']].to_csv('submission.csv',index=False)\n",
    "pd.set_option('max_colwidth', 60)\n",
    "test.sample(25)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4869.377734,
   "end_time": "2022-02-17T22:35:53.596464",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2022-02-17T21:14:44.218730",
   "version": "2.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
